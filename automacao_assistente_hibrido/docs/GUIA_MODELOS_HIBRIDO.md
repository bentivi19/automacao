# ü§ñ Assistente H√≠brido - Local + GPT-4o

## üìã Vis√£o Geral

O assistente agora suporta **2 modelos de IA**:

1. **Local - LLaVA (Ollama)** üìç
   - Roda no seu computador
   - Sem custos
   - Mais r√°pido
   - Suporta imagens e PDFs nativamente

2. **Nuvem - GPT-4o-mini** ‚òÅÔ∏è
   - Roda na nuvem OpenAI
   - Requer API key (pago, mas com plano gratuito)
   - Mais poderoso e criativo
   - Melhor para texto e an√°lise

---

## ‚öôÔ∏è Configura√ß√£o

### **Op√ß√£o 1: Usar Apenas Local (Padr√£o)**

1. Certifique-se que **Ollama est√° rodando**:
   ```powershell
   ollama serve
   ```

2. **Abra o Streamlit** (sem fazer mais nada):
   ```powershell
   streamlit run assistant.py
   ```

3. **Na sidebar**, voc√™ ver√° apenas:
   - "Local - LLaVA (Ollama)"

‚úÖ **Pronto! Tudo funcionando.**

---

### **Op√ß√£o 2: Usar Local + GPT-4o**

1. **Obtenha a chave OpenAI**:
   - V√° para: https://platform.openai.com/api-keys
   - Clique "Create new secret key"
   - Copie a chave (tipo: `sk-proj-...`)

2. **Configure no `.env`**:
   ```
   OPENAI_API_KEY=sk-proj-sua-chave-aqui
   ```

3. **Reinicie o Streamlit**:
   ```powershell
   streamlit run assistant.py
   ```

4. **Na sidebar**, voc√™ ver√° agora:
   - "Local - LLaVA (Ollama)"
   - "Nuvem - GPT-4o-mini" ‚Üê novo!

‚úÖ **Pronto! Ambos dispon√≠veis.**

---

## üéØ Como Usar

### **Seletor de Modelo**

Na **aba Config** da sidebar:

```
‚öôÔ∏è Configura√ß√µes
  ü§ñ Modelo de IA
  [ Escolha o modelo ‚ñº ]
  - Local - LLaVA (Ollama)
  - Nuvem - GPT-4o-mini
```

Clique para alternar entre modelos. Sua escolha √© **salva na sess√£o**.

### **Diferen√ßas Pr√°ticas**

**Local (LLaVA):**
```
Voc√™: "Analise esta imagem"
Modelo: Processa localmente, responde em 5-10 segundos
Custo: R$ 0,00
```

**Nuvem (GPT):**
```
Voc√™: "Escreva um artigo sobre Python"
Modelo: Resposta mais criativa e detalhada
Custo: ~R$ 0,01-0,05 por pergunta (muito barato)
```

---

## üíª Arquitetura Modular

### **model_handlers.py** - O cora√ß√£o do sistema

```python
# Interface unificada
class ModelHandler:
    def generate(prompt: str, img_data: bytes) -> str:
        raise NotImplementedError

# Implementa√ß√µes
class OllamaLocalHandler(ModelHandler):
    # Chama Ollama via HTTP
    
class GPTHandler(ModelHandler):
    # Chama OpenAI via SDK

# Gerenciador
class HybridModelManager:
    def generate(model_choice, prompt, img_data):
        # Roteia para o modelo certo
```

### **assistant.py** - Integra√ß√£o simples

```python
from model_handlers import model_manager

# Na sidebar
model_choice = st.selectbox("Escolha:", 
    options=model_manager.get_model_options()
)

# Na pergunta
resultado = model_manager.generate(
    model_choice, 
    prompt, 
    img_data
)
```

---

## üìä Compara√ß√£o de Modelos

| Aspecto | LLaVA Local | GPT-4o-mini |
|---------|-------------|------------|
| **Custo** | Gr√°tis | ~$0.15/1K tokens |
| **Velocidade** | 5-10s | 2-5s |
| **Criatividade** | Boa | Excelente |
| **An√°lise de Imagens** | ‚úÖ Sim | ‚úÖ Sim |
| **Offline** | ‚úÖ Sim | ‚ùå N√£o |
| **Qualidade Texto** | Boa | Excelente |
| **Requer API Key** | ‚ùå N√£o | ‚úÖ Sim |

---

## üîß Troubleshooting

### **"Modelo n√£o reconhecido"**
- Verifique se o modelo selecionado √© exato
- Reinicie o Streamlit

### **"Ollama n√£o est√° rodando"**
```powershell
# Terminal 1
ollama serve

# Terminal 2 (Streamlit)
streamlit run assistant.py
```

### **"OPENAI_API_KEY n√£o configurada"**
- Verifique que copiou certo no `.env`
- A chave deve come√ßar com `sk-proj-`
- Reinicie o Streamlit ap√≥s editar `.env`

### **GPT retorna erro de quota**
- Voc√™ pode ter ultrapassado o free tier
- Verifique seu usage em: https://platform.openai.com/account/usage/overview
- Consideramos usar apenas Local para economizar

---

## üí° Dicas de Uso

‚úÖ **Combine os modelos:**
- Use **Local** para tarefas r√°pidas (resumos, lembretes)
- Use **GPT** para tarefas criativas (artigos, brainstorm)

‚úÖ **Economize cr√©dito GPT:**
- Deixe Local como padr√£o
- Use GPT apenas quando precisar

‚úÖ **Aproveite o contexto de notas:**
- Ambos os modelos usam o hist√≥rico automaticamente
- Quanto melhor as notas, melhor a resposta

---

## üìù Exemplo Real

**Cen√°rio:** Pesquisa sobre Python

```
1. Local (LLaVA):
   "O que √© Python?"
   ‚Üí Resposta r√°pida e b√°sica

2. Nuvem (GPT):
   "Escreva um guia completo sobre Python"
   ‚Üí Resposta detalhada, artigo pronto

3. Local (LLaVA):
   "Resuma o artigo acima"
   ‚Üí Resume em segundos (usa contexto de notas)
```

---

## üöÄ Pr√≥ximas Melhorias (Futuro)

- [ ] Suporte a Claude (Anthropic)
- [ ] Suporte a Gemini (Google)
- [ ] Cache de respostas para economizar
- [ ] Hist√≥rico de qual modelo foi usado em cada nota
- [ ] Comparar respostas dos 2 modelos lado a lado

---

**Desenvolvido para m√°xima flexibilidade e modularidade!** üéâ
